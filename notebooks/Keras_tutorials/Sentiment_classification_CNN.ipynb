{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence classification\n",
    "- http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "- https://github.com/bhaveshoswal/CNN-text-classification-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "- Movie review data from Rotten Tomatoes (http://www.cs.cornell.edu/people/pabo/movie-review-data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_helpers import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, vocabulary, vocabulary_inv = load_data()\n",
    "\n",
    "# x.shape -> (10662, 56)\n",
    "# y.shape -> (10662, 2)\n",
    "# len(vocabulary) -> 18765\n",
    "# len(vocabulary_inv) -> 18765\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train.shape -> (8529, 56)\n",
    "# y_train.shape -> (8529, 2)\n",
    "# X_test.shape -> (2133, 56)\n",
    "# y_test.shape -> (2133, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5101, 10576, 12028, 17798, 13159,  4196,   292,   474, 17948,\n",
       "         475,  9570, 14759,  1059, 18393,  1080, 14902, 16433,  6957,\n",
       "         480,  4963, 16900,  7972, 16683,  9766, 17792, 11488,   480,\n",
       "        2171,  5138,   474,  6771, 14530,   475,   473,   473,   473,\n",
       "         473,   473,   473,   473,   473,   473,   473,   473,   473,\n",
       "         473,   473,   473,   473,   473,   473,   473,   473,   473,\n",
       "         473,   473])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PAD/>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_inv[473]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = x.shape[1] # 56\n",
    "vocabulary_size = len(vocabulary_inv) # 18765\n",
    "embedding_dim = 128\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 64\n",
    "drop = 0.5\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model design\n",
    "Keras에는 두 가지의 모델 생성 방법이 있습니다.\n",
    "\n",
    "1. Sequential Models\n",
    "2. Functional Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential model API**는 상당히 쉽게 딥러닝 모델을 생성하는 인터페이스를 제공하지만 한 방향성으로만 모델을 생성시킨다는 단점이 있습니다. 따라서 다음의 경우에는 Sequential model API로 모델을 생성하기가 어렵습니다.\n",
    "\n",
    "1. 다중의 입력 소스를 만들 경우\n",
    "2. 다중의 출력 층을 만들 경우\n",
    "3. 층을 여러 방향으로 공유하는 경우 등."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또 다른 방법은 **Functional model API**를 이용하는 것입니다. 이 방법은 좀 더 유연하게 딥러닝 모델을 디자인할 수 있게 합니다.\n",
    "만드는 것은 전혀 어렵지 않습니다. `keras.models.Model`을 활용하여 생성할 수 있으며 **Input**과 **Output**만 잘 정의해주면 됩니다.\n",
    "\n",
    "**Functional model API**에 대한 자세한 가이드는 Keras 공식 문서 (https://keras.io/getting-started/functional-api-guide/)를 참고하시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 **Functional model API**로 모델을 생성한 경우입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=2, activation='softmax')(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1:0\", shape=(?, 56), dtype=int32)\n",
      "Tensor(\"embedding_1/GatherV2:0\", shape=(?, 56, 128), dtype=float32)\n",
      "Tensor(\"reshape_1/Reshape:0\", shape=(?, 56, 128, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(embedding)\n",
    "print(reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Model...\n",
      "Train on 8529 samples, validate on 2133 samples\n",
      "Epoch 1/10\n",
      "8529/8529 [==============================] - 11s 1ms/step - loss: 0.6927 - acc: 0.5187 - val_loss: 0.6877 - val_acc: 0.5781\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.57806, saving model to weights.001-0.5781.hdf5\n",
      "Epoch 2/10\n",
      "8529/8529 [==============================] - 12s 1ms/step - loss: 0.6774 - acc: 0.6080 - val_loss: 0.6813 - val_acc: 0.5856\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.57806 to 0.58556, saving model to weights.002-0.5856.hdf5\n",
      "Epoch 3/10\n",
      "8529/8529 [==============================] - 13s 2ms/step - loss: 0.6577 - acc: 0.6833 - val_loss: 0.6692 - val_acc: 0.6409\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.58556 to 0.64088, saving model to weights.003-0.6409.hdf5\n",
      "Epoch 4/10\n",
      "8529/8529 [==============================] - 13s 2ms/step - loss: 0.6274 - acc: 0.7485 - val_loss: 0.6476 - val_acc: 0.6957\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.64088 to 0.69573, saving model to weights.004-0.6957.hdf5\n",
      "Epoch 5/10\n",
      "8529/8529 [==============================] - 13s 2ms/step - loss: 0.5794 - acc: 0.7940 - val_loss: 0.6112 - val_acc: 0.7056\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.69573 to 0.70558, saving model to weights.005-0.7056.hdf5\n",
      "Epoch 6/10\n",
      "8529/8529 [==============================] - 14s 2ms/step - loss: 0.5081 - acc: 0.8381 - val_loss: 0.5663 - val_acc: 0.7248\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.70558 to 0.72480, saving model to weights.006-0.7248.hdf5\n",
      "Epoch 7/10\n",
      "8529/8529 [==============================] - 16s 2ms/step - loss: 0.4227 - acc: 0.8688 - val_loss: 0.5261 - val_acc: 0.7323\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.72480 to 0.73230, saving model to weights.007-0.7323.hdf5\n",
      "Epoch 8/10\n",
      "8529/8529 [==============================] - 18s 2ms/step - loss: 0.3428 - acc: 0.8973 - val_loss: 0.5004 - val_acc: 0.7478\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.73230 to 0.74777, saving model to weights.008-0.7478.hdf5\n",
      "Epoch 9/10\n",
      "8529/8529 [==============================] - 20s 2ms/step - loss: 0.2728 - acc: 0.9259 - val_loss: 0.4908 - val_acc: 0.7525\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.74777 to 0.75246, saving model to weights.009-0.7525.hdf5\n",
      "Epoch 10/10\n",
      "8529/8529 [==============================] - 20s 2ms/step - loss: 0.2164 - acc: 0.9464 - val_loss: 0.4937 - val_acc: 0.7515\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.75246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2321869438>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"Traning Model...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_test, y_test))  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t1\tmore of a career curio than a major work <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "1\t1\tthis movie seems to have been written using mad libs there can be no other explanation hilariously inept and ridiculous <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "0\t0\tas surreal as a dream and as detailed as a photograph , as visually dexterous as it is at times imaginatively overwhelming <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "1\t0\there , adrian lyne comes as close to profundity as he is likely to get <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "1\t1\tthe cumulative effect of the movie is repulsive and depressing <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(len(X_test))\n",
    "    x_test = X_test[idx].reshape(1,56)\n",
    "    y_label = y_test[idx][0]\n",
    "    y_pred = model.predict(x_test)[0][0]\n",
    "    sent = \" \".join([vocabulary_inv[x] for x in x_test[0].tolist() if x != 0])\n",
    "    print(\"%.0f\\t%d\\t%s\" % (y_pred, y_label, sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
