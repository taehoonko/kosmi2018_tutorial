{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-entity recognition with Bidirectional LSTM-CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paper: https://arxiv.org/abs/1511.08308\n",
    "- Keras code: https://github.com/kamalkraj/Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs\n",
    "- Biomedical NER Data: https://github.com/cambridgeltl/MTL-Bioinformatics-2016/tree/master/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from validation import compute_f1\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from prepro import readfile,createBatches,createMatrices,iterate_minibatches,addCharInformatioin,padding\n",
    "from keras.utils import Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    for i,data in enumerate(dataset):    \n",
    "        tokens, casing,char, labels = data\n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pred = model.predict([tokens, casing,char], verbose=False)[0]   \n",
    "        pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        b.update(i)\n",
    "    return predLabels, correctLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences = readfile(\"data/train.tsv\")\n",
    "devSentences = readfile(\"data/devel.tsv\")\n",
    "testSentences = readfile(\"data/test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'O\\n'],\n",
       " ['positive', 'O\\n'],\n",
       " ['control', 'O\\n'],\n",
       " ['for', 'O\\n'],\n",
       " ['DMT1', 'O\\n'],\n",
       " ['up', 'O\\n'],\n",
       " ['-', 'O\\n'],\n",
       " ['regulation', 'O\\n'],\n",
       " ['was', 'O\\n'],\n",
       " ['a', 'O\\n'],\n",
       " ['murine', 'O\\n'],\n",
       " ['model', 'O\\n'],\n",
       " ['of', 'O\\n'],\n",
       " ['dietary', 'B-Disease\\n'],\n",
       " ['iron', 'I-Disease\\n'],\n",
       " ['deficiency', 'I-Disease\\n'],\n",
       " ['that', 'O\\n'],\n",
       " ['demonstrated', 'O\\n'],\n",
       " ['greatly', 'O\\n'],\n",
       " ['increased', 'O\\n'],\n",
       " ['levels', 'O\\n'],\n",
       " ['of', 'O\\n'],\n",
       " ['duodenal', 'O\\n'],\n",
       " ['DMT1', 'O\\n'],\n",
       " ['(', 'O\\n'],\n",
       " ['IRE', 'O\\n'],\n",
       " [')', 'O\\n'],\n",
       " ['mRNA', 'O\\n'],\n",
       " ['.', 'O\\n']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences = addCharInformatioin(trainSentences)\n",
    "devSentences = addCharInformatioin(devSentences)\n",
    "testSentences = addCharInformatioin(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', ['T', 'h', 'e'], 'O\\n'],\n",
       " ['positive', ['p', 'o', 's', 'i', 't', 'i', 'v', 'e'], 'O\\n'],\n",
       " ['control', ['c', 'o', 'n', 't', 'r', 'o', 'l'], 'O\\n'],\n",
       " ['for', ['f', 'o', 'r'], 'O\\n'],\n",
       " ['DMT1', ['D', 'M', 'T', '1'], 'O\\n'],\n",
       " ['up', ['u', 'p'], 'O\\n'],\n",
       " ['-', ['-'], 'O\\n'],\n",
       " ['regulation', ['r', 'e', 'g', 'u', 'l', 'a', 't', 'i', 'o', 'n'], 'O\\n'],\n",
       " ['was', ['w', 'a', 's'], 'O\\n'],\n",
       " ['a', ['a'], 'O\\n'],\n",
       " ['murine', ['m', 'u', 'r', 'i', 'n', 'e'], 'O\\n'],\n",
       " ['model', ['m', 'o', 'd', 'e', 'l'], 'O\\n'],\n",
       " ['of', ['o', 'f'], 'O\\n'],\n",
       " ['dietary', ['d', 'i', 'e', 't', 'a', 'r', 'y'], 'B-Disease\\n'],\n",
       " ['iron', ['i', 'r', 'o', 'n'], 'I-Disease\\n'],\n",
       " ['deficiency',\n",
       "  ['d', 'e', 'f', 'i', 'c', 'i', 'e', 'n', 'c', 'y'],\n",
       "  'I-Disease\\n'],\n",
       " ['that', ['t', 'h', 'a', 't'], 'O\\n'],\n",
       " ['demonstrated',\n",
       "  ['d', 'e', 'm', 'o', 'n', 's', 't', 'r', 'a', 't', 'e', 'd'],\n",
       "  'O\\n'],\n",
       " ['greatly', ['g', 'r', 'e', 'a', 't', 'l', 'y'], 'O\\n'],\n",
       " ['increased', ['i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 'd'], 'O\\n'],\n",
       " ['levels', ['l', 'e', 'v', 'e', 'l', 's'], 'O\\n'],\n",
       " ['of', ['o', 'f'], 'O\\n'],\n",
       " ['duodenal', ['d', 'u', 'o', 'd', 'e', 'n', 'a', 'l'], 'O\\n'],\n",
       " ['DMT1', ['D', 'M', 'T', '1'], 'O\\n'],\n",
       " ['(', ['('], 'O\\n'],\n",
       " ['IRE', ['I', 'R', 'E'], 'O\\n'],\n",
       " [')', [')'], 'O\\n'],\n",
       " ['mRNA', ['m', 'R', 'N', 'A'], 'O\\n'],\n",
       " ['.', ['.'], 'O\\n']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelSet = set()\n",
    "words = {}\n",
    "\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for token,char,label in sentence:\n",
    "            labelSet.add(label)\n",
    "            words[token.lower()] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Create a mapping for the labels ::\n",
    "label2Idx = {}\n",
    "for label in labelSet:\n",
    "    label2Idx[label] = len(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Hard coded case lookup ::\n",
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Read in word embeddings ::\n",
    "word2Idx = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'PADDING_TOKEN' and 'UNKNOWN_TOKEN' to word2Idx\n",
    "word2Idx['PADDING_TOKEN'] = 0\n",
    "word2Idx['UNKNOWN_TOKEN'] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리 학습한 Word2Vec 파일\n",
    "pretrained_w2v_file = '../../PubMed-w2v.bin'\n",
    "\n",
    "# 미리 학습한 Word2Vec 파일 읽기\n",
    "pretrained_w2v = gensim.models.KeyedVectors.load_word2vec_format(pretrained_w2v_file, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 0\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(words)+2, embedding_dim))\n",
    "for word, i in words.items():\n",
    "    word2Idx[word] = len(word2Idx)\n",
    "    if word in pretrained_w2v.vocab:\n",
    "        embedding_matrix[i] = pretrained_w2v.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2Idx = {\"PADDING\":0, \"UNKNOWN\":1}\n",
    "for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_<>()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|\":\n",
    "    char2Idx[c] = len(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = padding(createMatrices(trainSentences,word2Idx,  label2Idx, case2Idx,char2Idx))\n",
    "dev_set = padding(createMatrices(devSentences,word2Idx, label2Idx, case2Idx,char2Idx))\n",
    "test_set = padding(createMatrices(testSentences, word2Idx, label2Idx, case2Idx,char2Idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2Label = {v: k for k, v in label2Idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch,train_batch_len = createBatches(train_set)\n",
    "dev_batch,dev_batch_len = createBatches(dev_set)\n",
    "test_batch,test_batch_len = createBatches(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_input = Input(shape=(None,),dtype='int32',name='words_input')\n",
    "words = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],  weights=[embedding_matrix], trainable=False)(words_input)\n",
    "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False)(casing_input)\n",
    "character_input=Input(shape=(None,52,),name='char_input')\n",
    "embed_char_out=TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= Dropout(0.5)(embed_char_out)\n",
    "conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout)\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(52))(conv1d_out)\n",
    "char = TimeDistributed(Flatten())(maxpool_out)\n",
    "char = Dropout(0.5)(char)\n",
    "output = concatenate([words, casing,char])\n",
    "output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25))(output)\n",
    "output = TimeDistributed(Dense(len(label2Idx), activation='softmax'))(output)\n",
    "model = Model(inputs=[words_input, casing_input,character_input], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 30) 2910        char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 52, 30) 0           char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 52, 30) 2730        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 1, 30)  0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, None, 30)     0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    1947800     words_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 30)     0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 238)    0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 400)    702400      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, None, 3)      1203        bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 2,657,107\n",
      "Trainable params: 709,243\n",
      "Non-trainable params: 1,947,864\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 1/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 2/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 3/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 4/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 5/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 6/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 7/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 8/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 9/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 10/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 11/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 12/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 13/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 14/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 15/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 16/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 17/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 18/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 22/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 23/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 24/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 25/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 26/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 27/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 28/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 29/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 30/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 31/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 32/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 33/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 34/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 35/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 36/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 37/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 38/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 39/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 40/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 41/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 42/50\n",
      "89/90 [============================>.] - ETA: 0s \n",
      "Epoch 43/50\n",
      "84/90 [===========================>..] - ETA: 2s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e0d1bfc85ea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_batch_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):    \n",
    "    print(\"Epoch %d/%d\"%(epoch,epochs))\n",
    "    a = Progbar(len(train_batch_len))\n",
    "    for i,batch in enumerate(iterate_minibatches(train_batch,train_batch_len)):\n",
    "        labels, tokens, casing,char = batch       \n",
    "        model.train_on_batch([tokens, casing,char], labels)\n",
    "        a.update(i)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "922/923 [============================>.] - ETA: 0sDev-Data: Prec: 0.607, Rec: 0.347, F1: 0.441\n"
     ]
    }
   ],
   "source": [
    "#   Performance on dev dataset        \n",
    "predLabels, correctLabels = tag_dataset(dev_batch)        \n",
    "pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, idx2Label)\n",
    "print(\"Dev-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_dev, rec_dev, f1_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939/940 [============================>.] - ETA: 0sTest-Data: Prec: 0.687, Rec: 0.297, F1: 0.415\n"
     ]
    }
   ],
   "source": [
    "#   Performance on test dataset       \n",
    "predLabels, correctLabels = tag_dataset(test_batch)        \n",
    "pre_test, rec_test, f1_test= compute_f1(predLabels, correctLabels, idx2Label)\n",
    "print(\"Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_test, rec_test, f1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
